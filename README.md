# cifar10_vanishing_gradients.py
Vanishing Gradient Experiment in Deep CNNs (PyTorch, CIFAR-10) A comparative study of Sigmoid vs ReLU activation functions in a 4-layer convolutional neural network. Includes training curves, per-layer gradient flow analysis, and activation gradient heatmaps to visualize vanishing gradients in deep networks.
# ğŸ”¬ CIFAR-10 Vanishing Gradient Experiment  
### Sigmoid vs ReLU in a 4-Layer Deep CNN (PyTorch)

This project investigates the **vanishing gradient problem** by training two identical CNNs on CIFAR-10 â€” one using **Sigmoid** activations and the other using **ReLU** â€” and comparing:

âœ… Training & test accuracy  
âœ… Loss curves  
âœ… Per-layer gradient magnitudes  
âœ… Activation gradient heatmaps  
âœ… Convergence speed & learning dynamics  

---

## ğŸ“Œ Why This Experiment?

The **vanishing gradient problem** makes deep networks hard to train when activation functions like **Sigmoid / Tanh** squash gradients toward zero.

ğŸ” In shallow networks â†’ **not a big issue**  
ğŸ“‰ In deeper networks â†’ **training collapses / becomes very slow**

This repo provides a **clean, visual, experiment-based explanation** instead of only theory.

---

## ğŸ§  Model Architecture (Same for Both)

| Layer | Type | Output Shape |
|-------|------|--------------|
| Conv1 | 3 â†’ 32 | 32Ã—32 |
| Conv2 | 32 â†’ 64 | 32Ã—32 |
| Conv3 | 64 â†’ 128 | 16Ã—16 â†’ 8Ã—8 |
| Conv4 | 128 â†’ 128 | 4Ã—4 |
| FC    | 128Ã—4Ã—4 â†’ 10 | logits |

ğŸ” Only **activation function changes**  
ğŸ”µ Model A â†’ Sigmoid  
ğŸŸ  Model B â†’ ReLU  

---

## ğŸ“Š Key Results

| Observation | Sigmoid | ReLU |
|-------------|---------|------|
| First few epochs | Slow start | Learns fast |
| Accuracy at Epoch 1 | ~45% | ~60% |
| Final Accuracy | ~72â€“74% | ~82â€“83% |
| Gradient flow | Shrinks layer-wise | Stable per layer |
| Convergence | Gradual | Rapid |

ğŸ“Œ Result: **ReLU trains faster and avoids vanishing gradients.**  
ğŸ“Œ Sigmoid eventually learns, but needs more epochs and loses accuracy.

---

## ğŸ“ˆ Plots & Visualizations

### âœ… Test Loss & Accuracy Curves
*(saved as `loss_acc_comparison.png`)*  
![Loss & Accuracy](save_dir/loss_acc_comparison.png)

### âœ… Per-Layer Gradient Norms
*(saved as `grad_norms_per_layer.png`)*  
![Gradient Norms](save_dir/grad_norms_per_layer.png)

### âœ… Activation Gradient Heatmap (Conv1)
*(saved as `activation_gradmaps_conv1.png`)*  
![Grad Heatmap](save_dir/activation_gradmaps_conv1.png)

---

## â–¶ï¸ How to Run

```bash
git clone https://github.com/maniwalunj03-bot/cifar10-vanishing-gradients.git
cd cifar10-vanishing-gradients
pip install -r requirements.txt
python cifar10_vanishing_gradients.py

ğŸ”¹ Automatically saves plots inside: cifar_vanish_results/
ğŸ”¹ Supports GPU & CPU
ğŸ”¹ Adjustable: epochs, learning rate, activation type

ğŸš€ Future Extensions

ğŸ”² Add Tanh activation
ğŸ”² Add BatchNorm to reduce internal covariate shift
ğŸ”² Increase depth: 6â€“10 layers to amplify vanishing gradient effects
ğŸ”² Train with SGD + Momentum for comparison
ğŸ”² Add cosine LR scheduler
ğŸ”² Try Swish / GELU activations

ğŸ‘©â€ğŸ’» Author

Manisha Walunj
Chemistry + Machine Learning | Deep Learning Research
ğŸ”— GitHub: https://github.com/maniwalunj03-bot

ğŸ”— LinkedIn: https://www.linkedin.com/in/manisha-walunj/

ğŸ“ License

MIT â€” free to use, modify, and cite.

ğŸ’¡ If You Use This Repo

Feel free to â­ star the project or tag me on LinkedIn â€” happy to connect!
